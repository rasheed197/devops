Welcome.

In this lecture, we will see some amazing S3 bucket features that will help us save costs in long run and follow disaster recovery compliance and few other things. I know this is getting more in depth, but this will be short and very essential for you to know as a DevOps engineer or architect.

So on your S3 bucket, go to `Management` tab and you will see here `Lifecycle rules`. So these rules define the transition of objects from one storage class to other based on its age. We can understand this by doing it. Click on `Create lifecycle rule`.

So before we get into this, why do you really need to transition object storage class from one class to another? This is to save costs. By default, we have stored it in standard Storage class when it is accessed frequently, but if it's not accessed frequently, why unnecessarily pay for it?

So we will give the name of the rule as `CostEffectiveTransitions`, it's really just a name. Now, you can `Apply that on all the objects` or you can `Limit` by giving a prefix, right? So any file that starts with the name image or whatever, those files only will be transitioned.

We'll say just apply all object. Again, this is based on your use case. Now, keep in mind, this bucket is versioned. So you should define rules for the current object and the non-current version object also if you want to save the cost, otherwise, there's no point in just transitioning the current version object because the old version still exists over there and it'll be still occupying the space.

So `Move current version object between storage class`, `Move non-current version` also, you can also `Expire object in a non-versioned bucket`, expire means delete, but in a versioned bucket, that means place a delete marker. So you also need to `Permanently delete non-current versions of the objects`. Again, this is based on age that we are going to define now. If there is an incomplete upload, you can also delete those objects.

So transition from, you know, current storage class, which is Standard to `Standard Infrequent Access` in how many days? Let's say `30` days. I'm just throwing some numbers over here. These numbers will be based on your use cases. And from Standard Infrequent Access to let's say `One Zone Infrequent Access`, which is much cheaper than that. So let's say after `60` days, 60 days age of the object and then `Glacier Flexible Retrieval`, this is formerly called as Glacier and I will say `90` days, again, it's cheaper. Then you have `Glacier Deep Archive`, which is the cheapest one, but if you want to transition to Glacier Deep Archive, it should be 90 days more than this time. So 90 plus 90, `180` I can give. 180 or more.

Okay, now this is for current version. I acknowledge this, but the previous versions also, I will do the same. So non-current version, I'll say move to `Standard Infrequent` after `35` days and `One Zone Infrequent Access` after `65` days, `Glacier`, I'll say `95` days, and `Glacier Deep Archive`, I will say `185` days. Okay, there should be 90 days difference in between this both. And I acknowledge. So version and non-version both.

Then I want to expire the object, let's say after `450` days, again, just a number, you need to decide what's right for you based on your project.

And then `Permanently delete non-current version` because this is going to again, place a delete marker because it's a versioned bucket. So I will say `455` days, so permanently all the objects will be deleted after 455 days.

And in between these times, it'll be transitioning from one storage class to another or cheaper one, till it expires after 455 days everything. `Incomplete upload`, I want to delete that also. Let's say after maybe just `15` days.

Okay, if everything is good, if your rules are good, if you click on `Create rule`, it will get created. Otherwise, it'll throw some error that you need to make this or that changes. So let's click on `Create rule`, and the rule is created.

So you can `Edit` the rules also later, you can `View details` or you can delete the lifecycle rule as well. So this is good for cost cutting, but then you have one more. Okay, let's go back to `Management`, one more very common requirement that you need to follow because of the compliance also is `disaster recovery`. What happens if everything is gone from the S3 bucket due to some disaster? Now, this is rare because we discussed about the zones, they are in different places in one region and we have our data in multiple zones, but a disaster is a disaster, and compliance is compliance.

So for disaster recovery, one very common thing is to have the data replicated to another region, the other part of the world, a different country, a different continent. So first, we'll do this, we'll go to a different region. Let's say I will select here `Oregon` and I will create a bucket in another region. I just named it as `barista908dr` and I'll simply create the bucket.

Okay, let's go back to our original bucket and we'll set a replication rule, `Create replication rule`. It's pretty simple and just give our replication rule, I'll say `DisasterRecoveryBarista908`, `Status: enabled`.

And now you can do it for `Replicate all the objects`, again based on your requirement or you can `Limit the scope by giving a filter`. Choose the `Destination` bucket, now, you can select a bucket in another account, if you select that, you need to give the account ID and the bucket path. We'll just do it in the same account. I will say `Browse`, and we'll select and we'll select the destination bucket, it's in different region you see there. If you don't see your bucket in a different region, if you don't see the name of the bucket, then just refresh the page. Sometimes it takes a little while. `Choose path`.

Now, the versioning needs to be enabled in the destination bucket. So you can do it from here also, or you can go to the S3 bucket and enable versioning. `IAM role`, we'll talk about IAM role later. Just click on `Create new IAM role`. This creates a permission on the destination bucket that source bucket can copy data in it. `Encryption`, by default, there is encryption, but if you want to do encryption with KMS, then you can do that. `Destination storage class`, now, you don't the destination bucket in the disaster recovery to be standard storage. It won't be accessed frequently. So you can change the storage class of it to like `One Zone Infrequent Access` or `Standard Infrequent Access` based again on your use case. So I will select that and there are a few other options. You can make the replication faster by paying extra, but for disaster recovery, really we don't do that. But if the use case is different, you need to replicate faster than you can use `Replication Time Control`. So within 15 minutes, 99.99% of the object will be replicated. Otherwise it takes time. If you need to copy the delete marker, you can do that also, you can generate the replication metrics, again, all this will cost you extra.

I will just keep all this option as it is and say `Save`. Okay, now, when you try to save, previously, the replication will start for the new object, the existing object, you need to do replication yourself, but now you have an option. You can replicate the existing object, don't do that, there will be charges for it. So again, if you have the use case to replicate it, then do that.

But for now, for practice, just keep it `Do not replicate existing object` and say `Submit`.

So in this case, existing object won't get replicated. The new objects that you upload will get replicated. Okay, so those are too many things I know on the S3, but since S3 is so popularly used and it's so much used as a DevOps or for the project, you'll always see this or any other storage service that you use, object storage, you'll have similar options.

So as a DevOps, it's essential for you to know these kinds of option, go through different tabs. Again, go through different options. Just read them, have it in the back of your mind. You don't need to know all the options of S3 bucket as a DevOps, but these are pretty commonly used options.

Go through `Metrics` also, you can generate metrics, replication metrics, storage class analysis, all those things. It's out of the scope of DevOps but go through it. It'll be helpful for you in the interview and making decisions.

`Access points`, this you will understand when we do VPC in AWS part two. You can have a private access to your S3 bucket within the VPC itself. This won't make any sense when I'm saying this, but when you learn VPC, then you will understand this point. That in AWS part two we'll see.

Okay, after this, just clean everything. Remove all the rules, delete all the buckets. I will show you how you can clean buckets.

First, let me remove these things. So delete `Lifecycle rule`, `Replication rule`.

Okay, so first, you need to clean the bucket

and then you can remove the bucket

or you can do it like this also.

Select the bucket and say delete the bucket.

First, you need to empty the bucket,

so you can click empty the bucket.

Permanently delete.

Exist, the bucket still exists. The bucket is empty.

Just now you can delete the bucket.

Give the name of the bucket, delete bucket.

Okay, like this, delete all the buckets.

And then that's it for this service.

We've learned quite a lot in S3.

So complete this and join me in the next lecture.


